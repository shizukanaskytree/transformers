{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# import debugpy; debugpy.listen(5678); debugpy.wait_for_client(); debugpy.breakpoint()\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model_path', type=str, help='The path to the model checkpoint', default='../pretrained-bert')\n",
    "# parser.add_argument('--optim_path', type=str, help='The path to the optimizer checkpoint', default='../pretrained-bert')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "# model_path = args.model_path\n",
    "# optim_path = args.optim_path\n",
    "\n",
    "model_path = '../pretrained-bert'\n",
    "optim_path = '../pretrained-bert'\n",
    "\n",
    "model_checkpoint_path = os.path.join(model_path, \"checkpoint-2\", \"pytorch_model.bin\")\n",
    "loaded_model = torch.load(model_checkpoint_path)\n",
    "# print(f\"type(loaded_model): {type(loaded_model)}\")\n",
    "# print(loaded_model.keys())\n",
    "\n",
    "optim_checkpoint_path = os.path.join(model_path, \"checkpoint-2\", \"optimizer.pt\")\n",
    "loaded_optim = torch.load(optim_checkpoint_path)\n",
    "\n",
    "\n",
    "# Double the encoder layers\n",
    "new_loaded_model = copy.deepcopy(loaded_model)  # Create a deep copy of the loaded_model\n",
    "\n",
    "for k, v in loaded_model.items():\n",
    "    if k.startswith('bert.encoder.layer'):\n",
    "        # Extract the layer number\n",
    "        layer_num = int(k.split('.')[3])\n",
    "\n",
    "        # Duplicate the layer\n",
    "        new_layer_num = layer_num + 1\n",
    "        new_layer_key = k.replace(f'.{layer_num}.', f'.{new_layer_num}.')\n",
    "        new_loaded_model[new_layer_key] = v\n",
    "\n",
    "# Insert the new layers after encoder 0\n",
    "insert_index = None\n",
    "for idx, (k, v) in enumerate(new_loaded_model.items()):\n",
    "    if k == 'bert.encoder.layer.0.output.LayerNorm.bias':\n",
    "        insert_index = idx + 1\n",
    "        break\n",
    "\n",
    "new_layers = []\n",
    "for k, v in new_loaded_model.items():\n",
    "    if k.startswith('bert.encoder.layer.0.'):\n",
    "        new_layer_key = k.replace('bert.encoder.layer.0.', 'bert.encoder.layer.1.')\n",
    "        new_layers.append((new_layer_key, copy.deepcopy(v)))\n",
    "\n",
    "if insert_index is not None:\n",
    "    new_loaded_model = dict(\n",
    "        list(new_loaded_model.items())[:insert_index] +\n",
    "        new_layers +\n",
    "        list(new_loaded_model.items())[insert_index:]\n",
    "    )\n",
    "\n",
    "for x in new_loaded_model.keys():\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Double the encoder layers\n",
    "# lst = []\n",
    "# for k, v in loaded_model.items():\n",
    "#     k_split = k.split('.')\n",
    "#     # print(k_split)\n",
    "#     ### k_split 的打印结果如下:\n",
    "#     ### https://www.notion.so/xiaofengwu/transformer-BERT-layers-22b2d218b328402ca47fbe0e7449e8e4?pvs=4\n",
    "#     ### ['bert', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'weight']\n",
    "#     ### ['bert', 'encoder', 'layer', '0', 'attention', 'self', 'query', 'bias']\n",
    "#     print(f\"{k_split}\")\n",
    "\n",
    "#     ########################################################\n",
    "#     ########################################################\n",
    "#     # 保存新的键值对在尾部有问题! 需要对应 optimizer 的 state_dict\n",
    "#     ########################################################\n",
    "#     ########################################################\n",
    "\n",
    "#     if k_split[1] == 'encoder' and k_split[2] == 'layers':\n",
    "#         l_id = int(k_split[3])\n",
    "#         k_split[3] = str(l_id + 1) ### 为了构造新的数字, 但是其他的部分都不变\n",
    "#         new_k = '.'.join(k_split) ### 重新拼接\n",
    "#         lst.append([new_k, v.clone()]) ### 保存新的键值对在尾部有问题! 需要对应 optimizer 的 state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in lst: ### 将**新的**键值对添加到原来的字典中\n",
    "#     loaded_model[k] = v ### 这里的键值对是新的键值对, 但是值是原来的值\n",
    "\n",
    "# # Save the modified model checkpoint\n",
    "# new_model_checkpoint_dir = os.path.join(model_path, \"doubled_checkpoint\", \"pytorch_model.bin\")\n",
    "# os.makedirs(new_model_checkpoint_dir, exist_ok=True)\n",
    "# new_model_checkpoint_path = os.path.join(new_model_checkpoint_dir, \"pytorch_model.bin\")\n",
    "# torch.save(loaded_model, new_model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #-------------------------------------------------------------\n",
    "\n",
    "# encoder_sentence_encoder_layers_1_num_params = 0\n",
    "# for k, v in loaded_model.items():\n",
    "#     if k.startswith(\"encoder.sentence_encoder.layers.1\"):\n",
    "#         encoder_sentence_encoder_layers_1_num_params += v.numel()\n",
    "\n",
    "# state = loaded_optim['state']\n",
    "\n",
    "# ### Double the optimizer state\n",
    "# new_state_dict = {}\n",
    "\n",
    "\n",
    "# ### loaded_optim is a dict, has `state`` and `param_groups` two keys\n",
    "# ### loaded_optim['state'] is a dict, has 26 kv pair, 0: {'step': 10, 'exp_avg': tensor, 'exp_avg_sq': tensor}\n",
    "# ### loaded_optim['param_groups'] is a list with 2 elements, each element is a dict, {'weight_decay': 0.0, 'lr': 4e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'correct_bias': True, 'initial_lr': 5e-05, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
    "\n",
    "# # for k, v in loaded_optim['state_dict'].items():\n",
    "# #     k_split = k.split('.')\n",
    "# #     if k_split[0] == 'param_groups':\n",
    "# #         k_split[1] = str(int(k_split[1]) + 1)\n",
    "# #     new_k = '.'.join(k_split)\n",
    "# #     new_state_dict[new_k] = v.clone()\n",
    "\n",
    "# # Save the modified optimizer checkpoint\n",
    "# # new_optim_checkpoint_path = os.path.join(model_path, \"doubled_checkpoint\", \"optimizer.pt\")\n",
    "\n",
    "# # torch.save({\n",
    "# #     'state_dict': new_state_dict,\n",
    "# #     'param_groups': loaded_optim_checkpoint['param_groups']\n",
    "# # }, new_optim_checkpoint_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
